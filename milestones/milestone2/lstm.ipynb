{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This model will serve as our first foray into time-series forecasting using LSTMs. We will be following [this tutorial](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/).\n",
    "\n",
    "The code will be broken into the following sections:\n",
    "\n",
    "```{raw}\n",
    "I. Data and Imports\n",
    "II. Data Processing\n",
    "    a. Cleaning data\n",
    "    b. Separating data into drives (drive_id)\n",
    "    c. Next-play feature\n",
    "III. Model Creation\n",
    "IV. Model Training\n",
    "V. Next Steps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/NFL_Play_by_Play_2009-2018_(v5).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory feature extraction code\n",
    "[col for col in data.columns.to_list() if \"pos\" in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.a Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only valid plays\n",
    "data = data[data['play_type'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with too many missing values\n",
    "data = data.dropna(axis = 1, thresh=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only useful columns\n",
    "useful_columns = ['game_id', 'yardline_100', 'quarter_seconds_remaining', 'half_seconds_remaining', \n",
    "                  'game_seconds_remaining', 'quarter_end', 'drive', 'sp', 'qtr', 'down', 'goal_to_go', \n",
    "                  'ydstogo', 'ydsnet', 'yards_gained', 'shotgun', 'no_huddle', 'home_timeouts_remaining', \n",
    "                  'defteam_timeouts_remaining','defteam_score','away_timeouts_remaining', \n",
    "                  'timeout', 'defteam_timeouts_remaining', 'total_home_score',  \n",
    "                  'posteam_timeouts_remaining', 'posteam_score', 'total_away_score', 'defteam_score',\n",
    "                  'score_differential', 'defteam_score_post',  'score_differential_post', 'touchdown', 'play_type']\n",
    "# data = data[useful_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For LaTeX formatting\n",
    "for col in useful_columns:\n",
    "    print(r\"\\item \" + r\"\\texttt{\" + col.replace(\"_\", r\"\\_\") + \"}\")\n",
    "    # print(r\"\\item \" + f\"\\texttt{col.replace(\"_\", r\"\\_\")}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_play_type(x):\n",
    "    if x == \"kickoff\" or x == \"punt\" or x == \"field_goal\" or x == \"extra_point\":\n",
    "        return 0    # Special Teams\n",
    "    elif x == \"pass\" or x == \"qb_spike\":\n",
    "        return 1    # pass\n",
    "    elif x == \"run\" or x == \"qb_kneel\":\n",
    "        return 2    # run\n",
    "    else:\n",
    "        return 3    # no play\n",
    "    \n",
    "# Classifying play type\n",
    "data[\"play_type\"] = data[\"play_type\"].apply(classify_play_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming data types are numeric\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down missing is likely due to undowned plays, such as kickoff, extra point, etc.\n",
    "data = data[~data[\"down\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechecking missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There arent many, so dropping remaining\n",
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.b. Separating data by drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique drive id\n",
    "data[\"game_id_str\"] = data[\"game_id\"].astype(\"str\")\n",
    "data[\"drive_str\"] = data[\"drive\"].astype('str')\n",
    "\n",
    "data[\"drive_id\"] = data[\"game_id_str\"].str.cat(data[\"drive_str\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping temporary columns I created\n",
    "data = data.drop([\"game_id\", \"game_id_str\", \"drive_str\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering columns\n",
    "col_order = [\"drive_id\"] + list(data.columns)[:-1]\n",
    "data = data[col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking drive_ids\n",
    "ids = list(data[\"drive_id\"].unique())\n",
    "ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of drives\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking shape\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: Takes 23 minutes\n",
    "\n",
    "# Splitting the dataframe by drive ID and storing each drive as its own numpy array. \n",
    "# Each drive frame has shape (?, 35), where the question mark represents the number of plays in the drive (varies from 1-34).\n",
    "# I also drop drive_id since it is non-numeric. I finally take the dataframe and insert it as a numpy array\n",
    "# Thus, each element in broken_data is an array of play vectors of length 35.\n",
    "\n",
    "broken_data = [data[data[\"drive_id\"] == i].drop(\"drive_id\", axis=1).to_numpy() for i in ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.b.i Buffering data for consistency\n",
    "\n",
    "The longest drive was 34 plays, so we need to have each \"drive\" frame be of shape (34, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the list\n",
    "drive_data = broken_data\n",
    "\n",
    "# Initializing some values\n",
    "MAX_DRIVES = 0                      # To store the longest drive (# plays); in this data, MAX_DRIVES = 34\n",
    "FEATURES = drive_data[0].shape[1]   # To store the num of features: 35\n",
    "\n",
    "# Finding the longest drive\n",
    "for drive in drive_data:\n",
    "    if drive.shape[0] > MAX_DRIVES:\n",
    "        MAX_DRIVES = drive.shape[0]\n",
    "\n",
    "# Extending each drive frame by buffer of 0s\n",
    "for i, drive in enumerate(drive_data):\n",
    "    rows = drive.shape[0]\n",
    "\n",
    "    # Pad with rows of 0s\n",
    "    if rows != MAX_DRIVES:\n",
    "        buffer = np.zeros((MAX_DRIVES-rows, FEATURES))  # Create an array of 0s to fit onto the data to ensure it is of shape (MAX_DRIVES=34, 35)\n",
    "        drive_data[i] = np.concatenate((buffer, drive)) # Concatenating the 0-padding and the drive data into one numpy array and storing it\n",
    "\n",
    "# Setting drive data to an NP.array\n",
    "drive_data = np.array(drive_data)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that the shape is (# Drives, # Plays in each Drive, # Features) = (58729, MAX_DRIVES, FEATURES)\n",
    "drive_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining data. Note the 0 padding and real data at the end.\n",
    "drive_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating x and y data\n",
    "# X data is all plays except the last play\n",
    "# Y data is the last play of the drive\n",
    "# TODO: Explore what this would look like if we ignored the last play of the drive (i.e. punt, FG, TD).\n",
    "    # x.append(drive[:-2])\n",
    "    # y.append(drive[-2])\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for drive in drive_data:\n",
    "    x.append(drive[:-1])\n",
    "    y.append(drive[-1])\n",
    "    \n",
    "# Saving x and y lists as np.arrays\n",
    "x = np.array(x)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.c Next Play Feature\n",
    "\n",
    "We will not use this yet, but the code is here. Note the data warnings in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated, but could be useful later\n",
    "# Requires that broken_data is a list of pd.DataFrames, not np.arrays\n",
    "\n",
    "play_pairs = []\n",
    "\n",
    "for drive in broken_data:\n",
    "    for i in range(len(drive)-1):\n",
    "        cur_play = drive.iloc[i,:].to_numpy()\n",
    "        next_play = drive.iloc[i+1, :].to_numpy()\n",
    "        play_pairs.append(np.array([cur_play, next_play]))\n",
    "\n",
    "np.array(play_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_pairs = np.array(play_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_pairs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Model Creation\n",
    "\n",
    "Here, we create a fairly standard LSTM model, which outputs vectors of shape (1, 35), matching the next-play in the sequence.\n",
    "\n",
    "We would like to further explore our optimizer and loss functions, as well as various model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM_DRIVES = 58279\n",
    "NUM_PLAYS = 33\n",
    "NUM_FEATURES = 35\n",
    "hidden_size = 128\n",
    "\n",
    "# Creating basic 2 layer LSTM\n",
    "model = Sequential([\n",
    "    layers.Input((NUM_PLAYS, NUM_FEATURES)), \n",
    "    layers.LSTM(hidden_size, recurrent_activation=\"tanh\", kernel_regularizer=\"l2\", return_sequences=True),\n",
    "    layers.LSTM(hidden_size, recurrent_activation=\"tanh\", kernel_regularizer=\"l2\"),\n",
    "    layers.Dense(NUM_FEATURES)\n",
    "])\n",
    "\n",
    "# TODO: Explore model params. Add momentum to optimizer? KL Divergence for loss?\n",
    "model.compile(optimizer='adam',\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=['accuracy', \"f1_score\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Model Training\n",
    "\n",
    "As you can see, the model trains quite well, achieving an accuracy of 54%.\n",
    "\n",
    "We would like to add validation data to the model to ensure that it is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(x=x, y=y, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experiment with various model architectures and frameworks\n",
    "   1. LSTM\n",
    "   2. GRU\n",
    "   3. Transformer\n",
    "   4. Encoder-Decoder\n",
    "2. Hyperparameter optimization\n",
    "   1. Loss function\n",
    "   2. Optimizer\n",
    "   3. Regularization\n",
    "   4. Weight normalization\n",
    "   5. Model architectures\n",
    "3. Dataset preparation\n",
    "   1. Normalization\n",
    "   2. Revisit feature selection\n",
    "   3. Look into time-series methods (`tf.keras.preprocessing.timeseries_dataset_from_array`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
