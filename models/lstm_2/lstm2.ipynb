{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This model will serve as our first foray into time-series forecasting using LSTMs. We will be following [this tutorial](https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/).\n",
    "\n",
    "The code will be broken into the following sections:\n",
    "\n",
    "```{raw}\n",
    "III. Model Creation\n",
    "IV. Model Training\n",
    "V. Next Steps\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from .h5 file\n",
    "\n",
    "# Load data from the HDF5 file\n",
    "with h5py.File('preprocessed_data.h5', 'r') as hf:\n",
    "    x = hf['X'][:]\n",
    "    y = hf['y'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Model Creation\n",
    "\n",
    "Here, we create a fairly standard LSTM model, which outputs vectors of shape (1, 35), matching the next-play in the sequence.\n",
    "\n",
    "We would like to further explore our optimizer and loss functions, as well as various model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.a Normalization Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_layer = layers.Normalization(axis=-1)\n",
    "norm_layer.adapt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Creating basic 2 layer LSTM\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m([\n\u001b[1;32m      8\u001b[0m     layers\u001b[38;5;241m.\u001b[39mInput((NUM_PLAYS, NUM_FEATURES)), \n\u001b[1;32m      9\u001b[0m     norm_layer, \n\u001b[1;32m     10\u001b[0m     layers\u001b[38;5;241m.\u001b[39mMasking(mask_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.1\u001b[39m),\n\u001b[1;32m     11\u001b[0m     layers\u001b[38;5;241m.\u001b[39mBatchNormalization(),\n\u001b[1;32m     12\u001b[0m     layers\u001b[38;5;241m.\u001b[39mLSTM(hidden_size, recurrent_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m, return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     13\u001b[0m     layers\u001b[38;5;241m.\u001b[39mBatchNormalization(),\n\u001b[1;32m     14\u001b[0m     layers\u001b[38;5;241m.\u001b[39mLSTM(hidden_size, recurrent_activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     15\u001b[0m     layers\u001b[38;5;241m.\u001b[39mBatchNormalization(),\n\u001b[1;32m     16\u001b[0m     layers\u001b[38;5;241m.\u001b[39mDense(NUM_FEATURES)\n\u001b[1;32m     17\u001b[0m ])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# TODO: Explore model params. Add momentum to optimizer? MSE because this feels like more of a regression problem.\u001b[39;00m\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     21\u001b[0m                 loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# NUM_DRIVES = 58279\n",
    "NUM_PLAYS = 21\n",
    "NUM_FEATURES = 35\n",
    "hidden_size = 128\n",
    "\n",
    "# Creating basic 2 layer LSTM\n",
    "model = Sequential([\n",
    "    layers.Input((NUM_PLAYS, NUM_FEATURES)), \n",
    "    layers.Masking(mask_value=-1.1),\n",
    "    norm_layer, \n",
    "    layers.BatchNormalization(),\n",
    "    layers.LSTM(hidden_size, recurrent_activation=\"tanh\", kernel_regularizer=\"l2\", return_sequences=True),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LSTM(hidden_size, recurrent_activation=\"tanh\", kernel_regularizer=\"l2\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dense(NUM_FEATURES)\n",
    "])\n",
    "\n",
    "# TODO: Explore model params. Add momentum to optimizer? MSE because this feels like more of a regression problem.\n",
    "model.compile(optimizer='adam',\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=['accuracy', \"MSE\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Model Training\n",
    "\n",
    "As you can see, the model trains quite well, achieving an accuracy of 54%.\n",
    "\n",
    "We would like to add validation data to the model to ensure that it is not overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m10022/10022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 26ms/step - MSE: 59089.7773 - accuracy: 0.5743 - loss: 59092.7148\n",
      "Epoch 2/5\n",
      "\u001b[1m10022/10022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 27ms/step - MSE: 1497.5549 - accuracy: 0.6428 - loss: 1502.5139\n",
      "Epoch 3/5\n",
      "\u001b[1m10022/10022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 29ms/step - MSE: 1547.7394 - accuracy: 0.6487 - loss: 1553.9034\n",
      "Epoch 4/5\n",
      "\u001b[1m10022/10022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m310s\u001b[0m 31ms/step - MSE: 1463.5544 - accuracy: 0.6487 - loss: 1469.0878\n",
      "Epoch 5/5\n",
      "\u001b[1m10022/10022\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m339s\u001b[0m 34ms/step - MSE: 1462.3384 - accuracy: 0.6557 - loss: 1467.4646\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(x=x, y=y, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experiment with various model architectures and frameworks\n",
    "   1. LSTM\n",
    "   2. GRU\n",
    "   3. Transformer\n",
    "   4. Encoder-Decoder\n",
    "2. Hyperparameter optimization\n",
    "   1. Loss function\n",
    "   2. Optimizer\n",
    "   3. Regularization\n",
    "   4. Weight normalization\n",
    "   5. Model architectures\n",
    "3. Dataset preparation\n",
    "   1. Normalization\n",
    "   2. Revisit feature selection\n",
    "   3. Look into time-series methods (`tf.keras.preprocessing.timeseries_dataset_from_array`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use None for first input shape for variable length sequence model inputs?\n",
    "- use preprocessing.pad_sequences\n",
    "- https://chatgpt.com/share/674f7ebb-778c-8011-a993-bd83320c73b8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
